{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "6f00e2234192f28c37a7eed5a00136f0f7305b656b202c591e67c7064728c6c1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Improving Computer Vision Accuracy using Convolutions (Human vs Horse)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "file download : https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "local_zip = './tmp/horse-or-human.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('./tmp/horse-or-human')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_horse_dir = os.path.join('./tmp/horse-or-human/horses')\n",
    "train_human_dir = os.path.join('./tmp/horse-or-human/humans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_horse_names = os.listdir(train_horse_dir)\n",
    "print(train_horse_names[:10])\n",
    "\n",
    "train_human_names = os.listdir(train_human_dir)\n",
    "print(train_human_names[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg \n",
    "\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "pic_index = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(ncols * 4 , nrows * 4)\n",
    "pic_index += 8\n",
    "next_horse_pix = [\n",
    "    os.path.join(train_horse_dir, fname)\n",
    "    for fname in train_horse_names[pic_index-8:pic_index]\n",
    "]\n",
    "\n",
    "next_human_pix = [\n",
    "    os.path.join(train_human_dir, fname)\n",
    "    for fname in train_human_names[pic_index-8:pic_index]\n",
    "]\n",
    "\n",
    "for i, img_path in enumerate(next_horse_pix+next_human_pix):\n",
    "    sp = plt.subplot(nrows, ncols, i+1)\n",
    "    sp.axis('Off')\n",
    "\n",
    "    img = mpimg.imread(img_path)\n",
    "    plt.imshow(img)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv2D(16,(3,3),activation='relu', input_shape=(300,300,3))\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(32,(3,3),activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512,activation='relu'),\n",
    "        tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=RMSprop(lr=0.001),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    './tmp/horse-or-human/',\n",
    "    target_size=(300,300),\n",
    "    batch_size=128,\n",
    "    class_mode='binary'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=8,\n",
    "    epochs=15,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "source": [
    "### Week4 Quiz\n",
    "\n",
    "1. Using Image Generator, how do you label images?\n",
    "- [ ] It's based on the file name\n",
    "- [X] It's based on the directory the images is contained in\n",
    "- [ ] You have to manually do It\n",
    "- [ ] TensorFlow figures it out from the contents\n",
    "\n",
    "2. What method on the images Generator is used to normalize the image?\n",
    "- [ ] normalize_image\n",
    "- [X] rescale\n",
    "- [ ] normalize\n",
    "- [ ] Rescale_image\n",
    "\n",
    "3. How did we specify the training size for the images?\n",
    "- [ ] The training_size parameter on the validation generator\n",
    "- [X] The target_size parameter on the training generator\n",
    "- [ ] The training_size parameter on the training generator\n",
    "- [ ] The target_size parameter on the validation generator\n",
    "\n",
    "4. When we specify the input_shape to be (300,300,3), what does that mean?\n",
    "- [ ] There will be 300 horses and 300 humans, loaded in batches of 3\n",
    "- [ ] \n",
    "Every Image will be 300x300 pixels, and there should be 3 Convolutional Layers\n",
    "- [X] Every Image will be 300x300 pixels, with 3 bytes to define color\n",
    "- [ ] There will be 300 images, each size 300, loaded in batches of 3\n",
    "\n",
    "5. If your training data is close to 1.000 accuracy, but your validation data isn’t, what’s the risk here?\n",
    "- [ ] You’re underfitting on your validation data\n",
    "- [X] You’re overfitting on your training data  \n",
    "- [ ] No risk, that’s a great result  \n",
    "- [ ] You’re overfitting on your validation data\n",
    "\n",
    "6. Convolutional Neural Networks are better for classifying images like horses and humans because:\n",
    "- [ ] \n",
    "In these images, the features may be in different parts of the frame\n",
    "- [ ] There’s a wide variety of horses\n",
    "- [ ] There’s a wide variety of humans\n",
    "- [X] All of the above\n",
    "\n",
    "7. After reducing the size of the images, the training results were different. Why?\n",
    "- [ ] \n",
    "There was more condensed information in the images\n",
    "- [ ] The training was faster\n",
    "- [X] We removed some convolutions to handle the smaller images\n",
    "- [ ] There was less information in the images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}